{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow模型的跨平台部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于公司由于业务发展需要，结合公司现有技术栈、性能等因素考虑，现需要将在Python投研环境通过TensorFlow训练的模型移植到C++实盘生产环境，连调研带采坑实现前前后后花了三四天，先将整个过程与遇到的坑整理为笔记，方便以后查阅。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上部署通过TensorFlow训练完成的模型方案有很多，仅仅是官网就提供了三种方案可选。\n",
    "\n",
    "- 使用其他语言的TensorFlow    \n",
    "包括C、C++、Java、Go、Swift等等。但是需要注意的是，存在一些令人困惑的现象，例如，如果你希望用C语言部署你的模型，那么你可能不用花很大的精力在编译TensorFlow在C中的动态链接库，如果你使用OSX，你甚至可以直接通过`brew install libtensorflow`一行命令完成动态链接库的安装与环境变量的导出。但是，官网没有提供任何C语言版本TensorFlow的文档，你只能通过动态库的头文件简短的注释去揣测每个API使用方法。而另一方面，使用C++版本的TensorFlow你需要手动从源代码尝试编译C++的动态库，这会遇到很多奇怪的问题，尤其当你是OSX用户时，这点之后的笔记会详细展开。由于Java、Go、Swift等没有尝试，这里就不再赘述了。\n",
    "\n",
    "- 使用saved_model_cli\n",
    "官网提供了较为详细的关于这个命令行工具的使用方法，大致是，如果你的系统Python环境，或者虚拟Python环境已经安装了TensorFlow，那么你可以通过这个工具轻而易举地从通过`SavedModelBuilder`这个模块保存的模型恢复并使用它在生产环境下工作，它同时支持通过命令行参数加载`.npy`类型的numpy数组直接作为模型的输入。由于官网有很详细的中文文档，所以这里也就不再赘述了。\n",
    "\n",
    "- 使用TensorFlow Serving\n",
    "Serving是一个TensorFlow专门用来做模型部署的模块，但是目前官网提供的文档还是英文的，在写这篇笔记时我还没有看完，按照计划应该是明天调研完毕。它支持将模型服务化，像一个Web服务一样运行在某个端口，然后通过gRPC与遵从RESTful-API的接口直接调用模型的预测功能，但是我还没有看完，希望明天能将这部分补全。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方案一：使用其他语言的TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从源代码编译TensorFlow的C++动态链接库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，需要从GitHub上的 [TensorFlow](https://github.com/tensorflow/tensorflow) 仓库Clone源代码到目标机器，命令：\n",
    "```\n",
    "git clone git@github.com:tensorflow/tensorflow.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对应于 Ubuntu 16.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，请确保以下依赖的对应版本已经正确安装：\n",
    "- eigen >= 3.3.3   \n",
    "  对于eigen，推荐通过源码安装，首先下载源码压缩包并解压：\n",
    "  ```\n",
    "  wget http://bitbucket.org/eigen/eigen/get/3.3.4.zip\n",
    "  ```\n",
    "  然后通过cmake生成MakeFile，注意eigen不允许In-Source builds，需要创建一个文件夹，可以起名为build，然后运行cmake命令：\n",
    "  ```\n",
    "  mkdir build && cd build\n",
    "  cmake .. && make\n",
    "  make install\n",
    "  ```\n",
    "- protobuf == 3.6.0    \n",
    "  对于Protobuf的版本是3.6.0，是在\n",
    "  ```\n",
    "  tensorflow-master/tensorflow/contrib/cmake/external/protobuf.cmake\n",
    "  ```\n",
    "  这个目录指定的，如果没有这个文件，请先运行`cmake .`指令。\n",
    "  对于protobuf的安装依然是推荐源码安装，安装过程基本同上：\n",
    "  ```\n",
    "  wget https://github.com/google/protobuf/releases/download/v3.6.0/protobuf-all-3.6.0.zip\n",
    "  ```\n",
    "  然后一次运行配置，编译、安装。\n",
    "  ```\n",
    "  ./configure\n",
    "  make\n",
    "  make install\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，在当前系统Python环境下，或者虚拟Python环境下的依赖也是必须的。\n",
    "```\n",
    "sudo apt-get install python-numpy python-dev python-pip python-wheel\n",
    "```\n",
    "如果以上指令出现权限问题或者环境变量未导出请自行考虑解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对应于 OSX 10.13.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流程完全相同，有几处不同如下：\n",
    "- eigen的安装请通过以下指令安装：\n",
    "```\n",
    "brew install --HEAD eigen\n",
    "```\n",
    "经过多次采坑和StackOverflow，确认是3.3.3和3.3.4版本的eigen存在一个C++模板偏特化的Bug，没有没有在Release版本中解决，请安装以上命令安装eigen，可以节省你一天时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，在当前系统Python环境下，或者虚拟Python环境下的依赖也是必须的。\n",
    "```\n",
    "pip install six numpy wheel \n",
    "brew install coreutils\n",
    "```\n",
    "如果以上指令出现权限问题或者环境变量未导出请自行考虑解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安装 Bazel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里由于Bazel官网有较为详细的教程，所以就不在此赘述了，请参阅：   \n",
    "- [Ubuntu 16.04 Bazel 安装教程](https://docs.bazel.build/versions/master/install-ubuntu.html)     \n",
    "- [OSX Bazel 安装教程](https://docs.bazel.build/versions/master/install-os-x.html)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先进入刚才TensorFlow的源码目录，执行以下命令：\n",
    "```\n",
    "./configure\n",
    "```\n",
    "然后根据交互式引导选择需要启用的编译选项，如是否启用CUDA等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编译C++动态库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请执行以下指令编译TensorFlow的C++动态库：\n",
    "```\n",
    "bazel build //tensorflow:libtensorflow_cc.so\n",
    "```\n",
    "编译完成后，务必执行一下操作：\n",
    "```\n",
    "cp -r bazel-genfiles/ /usr/local/include/\n",
    "cp -r tensorflow /usr/local/include/\n",
    "cp -r third_party /usr/local/include/\n",
    "cp -r bazel-bin/libtensorflow_cc.so /usr/local/lib/\n",
    "cp -r bazel-bin/libtensorflow_framework.so /usr/local/lib/\n",
    "```\n",
    "至此，C++版本的TensorFlow动态库就编译完成了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证动态链接库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以新建一个C++项目，通过官网C++的API文档的一个 [简单例子](https://www.tensorflow.org/api_guides/cc/guide) 验证C++版本的TensorFlow是否正常运行。\n",
    "```\n",
    "#include \"tensorflow/cc/client/client_session.h\"\n",
    "#include \"tensorflow/cc/ops/standard_ops.h\"\n",
    "#include \"tensorflow/core/framework/tensor.h\"\n",
    "\n",
    "int main() {\n",
    "  using namespace tensorflow;\n",
    "  using namespace tensorflow::ops;\n",
    "  Scope root = Scope::NewRootScope();\n",
    "  // Matrix A = [3 2; -1 0]\n",
    "  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\n",
    "  // Vector b = [3 5]\n",
    "  auto b = Const(root, { {3.f, 5.f} });\n",
    "  // v = Ab^T\n",
    "  auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\n",
    "  std::vector<Tensor> outputs;\n",
    "  ClientSession session(root);\n",
    "  // Run and fetch v\n",
    "  TF_CHECK_OK(session.Run({v}, &outputs));\n",
    "  // Expect outputs[0] == [19; -3]\n",
    "  LOG(INFO) << outputs[0].matrix<float>();\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "如果程序运行正常，那么结果会输出19和-3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用saved_model_builder进行模型持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么不使用checkpoint？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Saver.restore()`需要提前建立好计算图，这在理论上是可行的，但是对于模型跨平台来说，成本和效率都存在问题，当模型趋于复杂，序列模型、深度卷积、复杂全连接以及种种超参数以及优化技术都需要两端完全匹配，就目前来看是得不偿失的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用saved_model_builder持久化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们采用`saved_model_builder`持久化模型，这种方式持久化的模型会将计算图结构及其参数持久化为Protobuf文件到磁盘，这种方法是可以直接跨平台，跨语言恢复模型的。本质上，`saved_model_builder`持久化模型的过程即是将计算图结构，及其描述信息，以及变量名及其值等一系列模型相关的信息序列化为Protobuf的过程，在这个过程中，我们首先需要构造一个`signature_def`，用来指导并标记一些关键计算图节点或者边缘的序列化过程，我们直接看代码：\n",
    "```\n",
    "session = tf.Session()\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, 1])\n",
    "y_input = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "fc1 = tf.layers.dense(x_input, 10, tf.nn.relu)\n",
    "fc2 = tf.layers.dense(fc1, 10, tf.nn.relu)\n",
    "\n",
    "y_predict = tf.layers.dense(fc2, 1)\n",
    "\n",
    "loss_func = tf.losses.mean_squared_error(labels=y_input, predictions=y_predict)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss_func)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造`signature_def`对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码初始化了一个Session，并计算图的结点和边缘信息，直觉地，这个简单的双层感知机模型用来做一个简单的回归问题，接下来，我们需要根据这个已经构造好的计算图调用如下API构造`signature_def`对象：\n",
    "```\n",
    "signature = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={\n",
    "        'x_input': tf.saved_model.utils.build_tensor_info(x_input),\n",
    "        'y_input': tf.saved_model.utils.build_tensor_info(y_input)\n",
    "    },\n",
    "    outputs={\n",
    "        'y_predict': tf.saved_model.utils.build_tensor_info(y_predict),\n",
    "        'loss_func': tf.saved_model.utils.build_tensor_info(loss_func)\n",
    "    },\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "```\n",
    "可以看到，这个API接收三个重要参数，其中`inputs`即是`tf.placeholder`，用于描述输入张量，`outputs`即是输出张量。在这里，可以看到我们分别用字符串`x_input`与`y_input`，`y_predict`与`loss_func`作为序列化后获取张量的键，而`tf.saved_model.utils.build_tensor_info`就是将张量转为protobuff结构的快捷方法。总结一下，我们用`tf.saved_model.signature_def_utils.build_signature_def`方法构造了`signature_def`对象，这个对象包含了计算图中输入与输出张量的键值对信息，键即是张量名，值即是protobuff结构的张量，用一个`method_name`键来描述功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还是直接看代码：\n",
    "```\n",
    "for step in range(2000):\n",
    "    session.run(optimizer, {\n",
    "        x_input: x_train,\n",
    "        y_input: y_train\n",
    "    })\n",
    "\n",
    "    if (step + 1) % 500 == 0:    \n",
    "        if os.path.exists(graph_save_dir):\n",
    "            shutil.rmtree(graph_save_dir)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(graph_save_dir)\n",
    "        builder.add_meta_graph_and_variables(session,\n",
    "                                             [tf.saved_model.tag_constants.SERVING],\n",
    "                                             {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n",
    "        builder.save()\n",
    "```\n",
    "这段代码一边进行训练，一边检查是否需要持久化模型，这里有两个坑：\n",
    "- 我们需要在每一次需要持久化模型时创建一个`SavedModelBuilder`，而不能在训练全局仅仅初始化一个`SavedModelBuilder`，由于某些原因，如果你这么做，你会在第二次`add_meta_graph_and_variables`时捕获到异常，大意是说计算图的元信息和变量已经添加到`SavedModelBuilder`了，不可以重复添加，而`add_meta_graph`也是在做相同的事情。   \n",
    "- 你需要在每一次需要持久化模型时清空模型持久化目录，如果你不这么做，你会在`builder.save()`时捕获异常，大意是提示持久化路径不为空，不可以写入，你需要手动清空，事实上，在后文我们介绍Serving时会提到，TensorFlow这么做的设计是希望我们有一个版本号文件夹描述模型持久化版本，但是出于某些原因，这个设计与我们的模型持久化规有冲突，例如，我们希望根据验证集的准确率变化来决定是否执行Early Stop，以及更新持久化模型，但是我们又不认为这时候该更新版本号等等。    \n",
    "然后来简单介绍一下`add_meta_graph_and_variables`这个API，这个API的上一行是实例化`SavedModelBuilder`，这个就不展开了，需要给定一个持久化目录。对于`add_meta_graph_and_variables`主要接受三个参数，第一个即是Session对象，它包含了计算图和变量的信息，第二个变量是一个Tags数组，我们可以根据需求来设定Tag，例如这里的例子即是设定一个用于Serving的模型，第三个参数非常重要，他是一个字典对象，它的一个重要键值对即是：\n",
    "```\n",
    "{tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature}\n",
    "```\n",
    "即`default_serving`与`signature_def`对象，顾名思义，这个键值对描述了在持久化时和恢复时我们如何找到计算图中的输入和输出节点。\n",
    "最后，万事俱备了，我们只需要调用`builder.save()`，来持久化我们的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python环境下的模型恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接看代码：\n",
    "```\n",
    "# Session.\n",
    "session = tf.Session()\n",
    "\n",
    "# Load meta graph.\n",
    "meta_graph_def = tf.saved_model.loader.load(session, [tf.saved_model.tag_constants.SERVING], graph_save_dir)  # type: tf.MetaGraphDef\n",
    "\n",
    "# Get signature.\n",
    "signature_def = meta_graph_def.signature_def\n",
    "signature = signature_def[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "\n",
    "# Get input tensor.\n",
    "x_input_tensor = signature.inputs['x_input'].name\n",
    "y_input_tensor = signature.inputs['y_input'].name\n",
    "\n",
    "# Get output tensor.\n",
    "y_predict_tensor = signature.outputs['y_predict'].name\n",
    "\n",
    "# Get loss func.\n",
    "loss_op = signature.outputs['loss_func'].name\n",
    "\n",
    "_, loss = session.run([y_predict_tensor, loss_op], {\n",
    "    x_input_tensor: x_train,\n",
    "    y_input_tensor: y_train,\n",
    "})\n",
    "\n",
    "logging.warning('Loss: {}'.format(loss))\n",
    "```\n",
    "这段代码首先初始化了一个Session对象，然后通过`tf.saved_model.loader.load`API加载`meta_graph_def`对象，这个API接受三个参数，第一个是Session，第二个是在持久化模型时设定的Tags，第三个是模型的持久化路径。`meta_graph_def`对象是一个已经被反序列化的protobuff对象，我们可以轻易地通过点语法来获取`signature_def`对象。`signature_def`对象已经在上文提到，这个对象包含了计算图中输入与输出张量的键值对信息，键即是张量名，值即是protobuff结构的张量，用一个`method_name`键来描述功能。这个对象在持久化时由键值对：\n",
    "```\n",
    "{tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n",
    "```\n",
    "来确定，而获取了`signature_def`对象，我们就可以获取在持久化前通过键值对定义的关键张量了，而正如代码所展示的，获取了关键张量，我们就可以提供输入，来通过Session计算模型预测结果，到此，python环境下的模型恢复就结束了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C++环境下的模型恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依然是直接看代码：\n",
    "```\n",
    "int main() {\n",
    "    using namespace tensorflow;\n",
    "    using namespace tensorflow::ops;\n",
    "\n",
    "    SessionOptions sessionOptions;\n",
    "    RunOptions runOptions;\n",
    "\n",
    "    SavedModelBundle bundle;\n",
    "    Status status;\n",
    "\n",
    "    status = LoadSavedModel(sessionOptions, runOptions, GraphDir, {kSavedModelTagServe}, &bundle);\n",
    "\n",
    "    if (!status.ok()) {\n",
    "        std::cerr << \"Load model failed, reason: \" << status.ToString() << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    Scope root = Scope::NewRootScope();\n",
    "    std::vector<Tensor> yPredict;\n",
    "\n",
    "    auto signature = bundle.meta_graph_def.signature_def().find(\"signature\")->second;\n",
    "    auto inputs = signature.inputs();\n",
    "    auto outputs = signature.outputs();\n",
    "\n",
    "    auto xInputTensor = inputs.find(\"x_input\")->second;\n",
    "    auto yPredictTensor = outputs.find(\"y_predict\")->second;\n",
    "\n",
    "    std::vector<double > x(X, X + 640);\n",
    "\n",
    "    Tensor input(DT_FLOAT, TensorShape({1, 640}));\n",
    "\n",
    "    std::copy_n(x.begin(), x.size(), input.flat<float>().data());\n",
    "\n",
    "    TF_CHECK_OK(bundle.session->Run({{xInputTensor.name(), input}}, {yPredictTensor.name()}, {}, &yPredict));\n",
    "\n",
    "    LOG(INFO) << yPredict[0].matrix<float>();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "对于C++的代码就不做过多解释了，原理大致与Python类似，最大的不同是C++不直接初始化一个Session，而是初始化一个`SavedModelBundle`引用，然后调用`LoadSavedModel`传`SavedModelBundle`引用于Tags信息恢复模型，恢复的模型Session将会是`SavedModelBundle`的一个成员变量。之后的套路也与python大致相同，即获取`signature_def`对象，然后通过持久化前定义的键值对获取关键张量，然后提供输入给Session就可以计算结果。至此C++的模型恢复也就到此结束了。而事实上这个过程是及其艰辛的，C++的TensorFlow的文档不尽完善，甚至找不到通过恢复模型而得到的`Session`对象如何使用，而官网也只有`ClientSession`这样更高级的对象的介绍，以及诸多关于protobuff在C++中的使用问题，当然更多的原因还是笔者的C++太菜，还希望各位多多包涵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方案二：使用saved_model_cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saved_model_cli`提供了一种通过命令行检查并恢复模型的机制，如果你的TensorFlow是通过pip安装的，那么`saved_model_cli`应该已经被一同安装，`saved_model_cli`主要有两个命令，一个是`show`，一个是`run`，假设我们已经按照方案一提到的`saved_model_builder`方式在某个路径有了持久化的模型，我们可以通过如下方式检查该模型的相关信息：\n",
    "```\n",
    "saved_model_cli show --dir=graph_dir\n",
    "The given SavedModel contains the following tag-sets:\n",
    "train\n",
    "```\n",
    "通过`show`命令，给出持久化模型的路径，`saved_model_cli`返回有效的`meta_graph_def`值对应的键，这个键即是我们在：\n",
    "```\n",
    "builder.add_meta_graph_and_variables(session,\n",
    "                                     [tf.saved_model.tag_constants.SERVING],\n",
    "                                     {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n",
    "```\n",
    "即在持久化模型前为当前元计算图和变量信息定义的Tags，事实上我们可以根据需要在持久化时定义多个Tags，但是我目前没有用到这个功能。我们可以通过以下命令检查持久化模型对应Tags的`signature_def`信息：\n",
    "```\n",
    "saved_model_cli show --dir=. --tag_set=serve --signatue_def=serving_default\n",
    "The given SavedModel SignatureDef contains the following input(s):\n",
    "  inputs['x_input'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: Placeholder:0\n",
    "  inputs['y_input'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: Placeholder_1:0\n",
    "The given SavedModel SignatureDef contains the following output(s):\n",
    "  outputs['loss_func'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: ()\n",
    "      name: mean_squared_error/value:0\n",
    "  outputs['y_predict'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: dense_2/BiasAdd:0\n",
    "Method name is: tensorflow/serving/predict\n",
    "```\n",
    "可以看到，我们在通过：\n",
    "```\n",
    "signature = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={\n",
    "        'x_input': tf.saved_model.utils.build_tensor_info(x_input),\n",
    "        'y_input': tf.saved_model.utils.build_tensor_info(y_input)\n",
    "    },\n",
    "    outputs={\n",
    "        'y_predict': tf.saved_model.utils.build_tensor_info(y_predict),\n",
    "        'loss_func': tf.saved_model.utils.build_tensor_info(loss_func)\n",
    "    },\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "```\n",
    "这种方式定义的`signature_def`信息可以通过上述命令检出，这里回顾以下`signature_def`，我们构造了`signature_def`对象，这个对象包含了计算图中输入与输出张量的键值对信息，键即是张量名，值即是protobuff结构的张量，用一个`method_name`键来描述功能。可以看到，在通过`save_model_cli`的`show`命令检出的`signature_def`信息中，我们定义的关键张量的键值对，都被成功检出。接着我们可以通过`run`命令恢复模型，并根据需要进行计算：\n",
    "```\n",
    "saved_model_cli run --dir=./graph --tag_set=serve --signature_def=serving_default --inputs \"x_input=./x_train.npy;y_input=./y_train.npy\"\n",
    "\n",
    "2018-07-21 16:23:38.098684: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX\n",
    "Result for output key loss_func:\n",
    "0.0157029\n",
    "Result for output key y_predict:\n",
    "[[ 0.94755262]\n",
    " [ 0.92479306]]\n",
    "```\n",
    "这里有两个大坑，需要格外注意：\n",
    "\n",
    "- Python3可以运行`run`命令，但是无法通过给定--inputs以`*.npy`文件的形式计算结果，如果你这么做，你会捕获到一个异常，大意是提示存在编解码的错误，简单谷歌后无法解决，遂搁置。   \n",
    "- Python2.7可以运行`run`命令，但是inputs后面的参数形式必须要用双引号\"\"包含，如果你这么做，那么会导致第一个之后的输入无法被解析。    \n",
    "\n",
    "针对第二个坑，官网例程中给出的例子并没有用双引号包含：\n",
    "```\n",
    "$ saved_model_cli run --dir /tmp/saved_model_dir --tag_set serve \\\n",
    "--signature_def x1_x2_to_y \\\n",
    "--inputs x1=/tmp/my_data1.npz[x];x2=/tmp/my_data2.pkl --outdir /tmp/out \\\n",
    "--overwrite\n",
    "Result for output key y:\n",
    "[[ 1.5]\n",
    " [ 2.5]\n",
    " [ 3.5]]\n",
    "```\n",
    "起初我以为是shell的问题，因为笔者的shell是zsh，但是切换回bash已然会报错，这点请务必注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方案三：使用TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Serving是一个专门用于将TensorFlow模型部署于生产环境一个工具模块，通过TensorFlow Serving，我们可以轻易地部署TensorFlow模型到生产环境。在介绍TensorFlow Serving的核心概念和使用方法前，请先安装依赖：\n",
    "```\n",
    "sudo apt-get update && sudo apt-get install -y \\\n",
    "        automake \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        libcurl3-dev \\\n",
    "        git \\\n",
    "        libtool \\\n",
    "        libfreetype6-dev \\\n",
    "        libpng12-dev \\\n",
    "        libzmq3-dev \\\n",
    "        pkg-config \\\n",
    "        python-dev \\\n",
    "        python-numpy \\\n",
    "        python-pip \\\n",
    "        software-properties-common \\\n",
    "        swig \\\n",
    "        zip \\\n",
    "        zlib1g-dev\n",
    "```\n",
    "以上的依赖是对应于Ubuntu 16.04版本，如果是其他发行版本的Linux，请自行解决依赖问题。在确认依赖安装后，请通过以下命令安装TensorFlow Serving的客户端Python API工具包：\n",
    "```\n",
    "pip install tensorflow-serving-api\n",
    "```\n",
    "然后，通过以下命令安装TensorFlow Serving的服务器包：\n",
    "```\n",
    "echo“deb [arch = amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal”| sudo tee /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "\n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add  - \n",
    "\n",
    "sudo apt-get update && sudo apt-get install tensorflow-model-server\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Servables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Servables是TensorFlow Serving的一个核心概念，我们可以直观的理解为一个服务，这个服务运行在某个端口，监听某个端口并处理请求，这个请求可以是gRPC协议的，也可以是HTTP的，请求处理完成后返回结果，直觉地，我们将某个模型的输入封装成gRPC或者HTTP的request，然后POST给Servables，然后Servables作为模型的抽象，接受输入并计算输出，将结果封装成gRPC或者HTTP的response，返回给客户端，这个过程与Web服务非常类似。本质上，Servables所作的核心工作，即是自动地读取我们已经持久化的模型，并通过代码模板自动地以C++版本恢复模型，并实现了一个灵活的服务，这个服务支持一个或者多个协议的请求，监听某个端口，专注地处理request并返回response。一个典型的Servables的核心即是一个`SavedModelBundle`实例，关于`SavedModelBundle`类，我们在介绍方案一时，在用C++恢复模型时简要的展示了这个类的用法：\n",
    "```\n",
    "SessionOptions sessionOptions;\n",
    "RunOptions runOptions;\n",
    "\n",
    "SavedModelBundle bundle;\n",
    "Status status;\n",
    "\n",
    "status = LoadSavedModel(sessionOptions, runOptions, GraphDir, {kSavedModelTagServe}, &bundle);\n",
    "\n",
    "bundle.session->Run({{xInputTensor.name(), input}}, {yPredictTensor.name()}, {}, &yPredict);\n",
    "```\n",
    "可以看到，`SavedModelBundle`的实例有一个`session`成员变量，这个`session`与Python版本的`Session`的功能是基本类似的。另一方面，我们在介绍方案一时曾经提到，Python版本的TensorFlow的`SavedModelBuilder`在调用`add_meta_graph_and_variables`时有一个小问题，即对于每一个`SavedModelBuilder`实例来说，你只能调用一次`add_meta_graph_and_variables`方法，如果你试图调用两次，那么你会捕获到一个异常，提示`meta_graph`和`variables`已经添加，不可以重复添加，除此之外，在`SavedModelBuilder`的某个实例多次调用`builder.save()`方法时，你仍会捕获一个异常，这个异常提示持久化目录不为空，如果你执意要持久化此模型，那么你必须先清空当前持久化目录才可以调用`builder.save()`方法。回顾这两个问题，是为了引出一个版本控制话题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Servable Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，Servables提供了一些机制支持不同版本的模型，在后文我们会看到，我们会通过不同的URI，例如不同的协议，不同的版本号：\n",
    "```\n",
    "http://host:port/v1/models/test/versions/${MODEL_VERSION}:predict\n",
    "```\n",
    "可以出发Servables调度不同版本的模型处理不同URI的request，即我们可以将不同的版本的模型按约定的版本号建立不同的文件夹存储在磁盘，通过TensorFlow Serving，我们可以非常自然地将他们组合起来使用而使得彼此互不影响，这可能非常适用于某一些生产环境的部署，但是目前就笔者的问题规模，暂时没有用到这个特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loader and Source and Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，Loader和Sources支持着不同版本Servables的实例化与调度，即Source将会根据特定的模型版本创建Loader，而Loader将会实例化Servables，这些Servables又统一受Manager管理，Manager更像一个反向代理的角色，它直接处理request，并分发给特定的Servables处理，并返回response。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建计算图并持久化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，仍然以方案一与方案二的例程为例，创建一个持久化模型，假设我们的网络具有如下结构：\n",
    "```\n",
    "session = tf.Session()\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, [None, 1])\n",
    "y_input = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "fc1 = tf.layers.dense(x_input, 10, tf.nn.relu)\n",
    "fc2 = tf.layers.dense(fc1, 10, tf.nn.relu)\n",
    "\n",
    "y_predict = tf.layers.dense(fc2, 1)\n",
    "\n",
    "loss_func = tf.losses.mean_squared_error(labels=y_input, predictions=y_predict)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss_func)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "```\n",
    "那我们首先需要构造`signature_def`对象，我们可以通过以下代码构造`signature_def`对象：\n",
    "```\n",
    "signature = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={\n",
    "        'x_input': tf.saved_model.utils.build_tensor_info(x_input),\n",
    "        'y_input': tf.saved_model.utils.build_tensor_info(y_input)\n",
    "    },\n",
    "    outputs={\n",
    "        'y_predict': tf.saved_model.utils.build_tensor_info(y_predict),\n",
    "        'loss_func': tf.saved_model.utils.build_tensor_info(loss_func)\n",
    "    },\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "```\n",
    "正如方案一中介绍的，我们这里简要的做一下回顾，API接收三个重要参数，其中`inputs`即是`tf.placeholder`，用于描述输入张量，`outputs`即是输出张量。在这里，可以看到我们分别用字符串`x_input`与`y_input`，`y_predict`与`loss_func`作为序列化后获取张量的键，而`tf.saved_model.utils.build_tensor_info`就是将张量转为protobuf结构的快捷方法。然后训练模型并持久化模型：\n",
    "```\n",
    "for step in range(2000):\n",
    "    session.run(optimizer, {\n",
    "        x_input: x_train,\n",
    "        y_input: y_train\n",
    "    })\n",
    "\n",
    "    if (step + 1) % 500 == 0:    \n",
    "        if os.path.exists(graph_save_dir):\n",
    "            shutil.rmtree(graph_save_dir)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(graph_save_dir)\n",
    "        builder.add_meta_graph_and_variables(session,\n",
    "                                             [tf.saved_model.tag_constants.SERVING],\n",
    "                                             {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n",
    "        builder.save()\n",
    "```\n",
    "我们构造`SavedModelBuilder`对象，并调用`add_meta_graph_and_variables`添加`meta_graph`与`variables`，并调用多次`builder.save()`方法持久化模型，其中`[tf.saved_model.tag_constants.SERVING]`是持久化模型的标签，我们后续可以通过命令行检查哪些标签有对应有效的持久化模型，`{tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature}`键值对指定了对于`SERVING`标签下`signature`键对应的计算图，我们在方案二中提到过，持久化模型可以对应多个计算图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查持久化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "持久化模型完成后，我们可以通过方案二介绍的命令行工具`saved_model_cli`检查持久化模型对应Tags的`signature_def`信息：：\n",
    "```\n",
    "saved_model_cli show --dir=. --tag_set=serve --signatue_def=serving_default\n",
    "The given SavedModel SignatureDef contains the following input(s):\n",
    "  inputs['x_input'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: Placeholder:0\n",
    "  inputs['y_input'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: Placeholder_1:0\n",
    "The given SavedModel SignatureDef contains the following output(s):\n",
    "  outputs['loss_func'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: ()\n",
    "      name: mean_squared_error/value:0\n",
    "  outputs['y_predict'] tensor_info:\n",
    "      dtype: DT_FLOAT\n",
    "      shape: (-1, 1)\n",
    "      name: dense_2/BiasAdd:0\n",
    "Method name is: tensorflow/serving/predict\n",
    "```\n",
    "可以看到，如果以上所有操作无误，应该可以看到`saved_model_cli`列出的模型信息应该与我们构造`signature_def`时一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用TensorFlow Serving API启动Serving服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将通过TensorFlow Serving API启动Serving服务：\n",
    "```\n",
    "tensorflow_model_server --port=9000 --rest_api_port=9001 --model_name=test --model_base_path=$(pwd)\n",
    "\n",
    "2018-07-24 20:11:09.772135: I tensorflow_serving/model_servers/main.cc:153] Building single TensorFlow model file config:  model_name: test model_base_path: /home/keyunlong/pycharm_project_775/playground/graph\n",
    "2018-07-24 20:11:09.772452: I tensorflow_serving/model_servers/server_core.cc:459] Adding/updating models.\n",
    "2018-07-24 20:11:09.772479: I tensorflow_serving/model_servers/server_core.cc:514]  (Re-)adding model: test\n",
    "2018-07-24 20:11:10.179066: I tensorflow_serving/core/basic_manager.cc:716] Successfully reserved resources to load servable {name: test version: 1}\n",
    "2018-07-24 20:11:10.179115: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: test version: 1}\n",
    "2018-07-24 20:11:10.179153: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: test version: 1}\n",
    "2018-07-24 20:11:10.179224: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /home/keyunlong/pycharm_project_775/playground/graph/1\n",
    "2018-07-24 20:11:10.179279: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/keyunlong/pycharm_project_775/playground/graph/1\n",
    "2018-07-24 20:11:10.199380: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
    "2018-07-24 20:11:10.331559: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\n",
    "2018-07-24 20:11:10.464002: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\n",
    "2018-07-24 20:11:10.466276: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 286961 microseconds.\n",
    "2018-07-24 20:11:10.466395: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /home/keyunlong/pycharm_project_775/playground/graph/1/assets.extra/tf_serving_warmup_requests\n",
    "2018-07-24 20:11:10.467172: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: test version: 1}\n",
    "2018-07-24 20:11:10.653258: I tensorflow_serving/model_servers/main.cc:323] Running ModelServer at 0.0.0.0:9000 ...\n",
    "2018-07-24 20:11:10.713689: I tensorflow_serving/model_servers/main.cc:333] Exporting HTTP/REST API at:localhost:9001 ...\n",
    "[evhttp_server.cc : 235] RAW: Entering the event loop ...\n",
    "```\n",
    "可以看到，我们通过`tensorflow_model_server`命令行工具，指定监听gRPC和HTTP的端口，指定模型名，指定模型基目录（支持多版本），就可以启动一个Serving服务了。通过Serving的启动日志我们可以发现，Serving成功的找到了版本号为1的模型，并通过`SavedModelBundle`类恢复模型，并通过Loader创建了一个Servable，然后开始监听9000和9001端口，进入事件循环，等待请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过gRPC调用Serving服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里仅仅展示gRPC的小程序，我们编写如下代码通过gRPC协议调用Serving服务：\n",
    "```\n",
    "# Init channel.\n",
    "channel = implementations.insecure_channel('localhost', 9000)\n",
    "# Init stub.\n",
    "stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "# Init request.\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'test'\n",
    "request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
    "request.inputs['x_input'].CopyFrom(\n",
    "    tf.contrib.util.make_tensor_proto(x_train, shape=x_train.shape)\n",
    ")\n",
    "request.inputs['y_input'].CopyFrom(\n",
    "    tf.contrib.util.make_tensor_proto(y_train, shape=y_train.shape)\n",
    ")\n",
    "# Predict.\n",
    "future = stub.Predict.future(request, 2.0)\n",
    "result = future.result().outputs['loss_func'].float_val\n",
    "logging.warning('Loss: {}'.format(result))\n",
    "```\n",
    "如果一切正常，你会看到以下数值作为输出，会因训练结果而异：\n",
    "```\n",
    "Connected to pydev debugger (build 181.4203.547)\n",
    "WARNING:root:Loss: [0.12157168239355087]\n",
    "```\n",
    "快速地解释一下这个小程序，我们构造了一个gRPC协议的请求，主要包含了模型名`test`，`signature_name`的键，即指定使用哪个计算图，构造了`x_input`和`y_input`张量，并封装成protobuf，然后通过实例化的程序桩调用Predict发送request，然后输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

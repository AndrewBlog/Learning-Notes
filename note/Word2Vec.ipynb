{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题设定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于One-hot的词向量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "I &= [1, 0, 0] \\\\\n",
    "Like &= [0, 1, 0] \\\\\n",
    "Apple &= [0, 0, 1] \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无法通过两向量夹角余弦值计算其相似度，word2vec提供了Skip-Gram（跳字模型）与CBOW（连续词袋模型）两个词嵌入模型，通过这种模型训练出的词向量可以较好的表示出词之间的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即跳字模型，其核心思想是对于一个上下文，设定一个大小为$m$的滑窗，在滑窗内选择$1$个中心词，预测滑窗内$m - 1$个背景词。即如果上下文是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "I\\  eat\\  apple\\  every\\  day\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每一个词进行One-hot编码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "I &= [1, 0, 0, 0, 0] \\\\ \n",
    "eat &= [0, 1, 0, 0, 0] \\\\\n",
    "apple &= [0, 0, 1, 0, 0] \\\\\n",
    "every &= [0, 0, 0, 1, 0] \\\\\n",
    "day &= [0, 0, 0, 0, 1]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定滑窗大小为$2$，如果选择中心词$apple$，那么将会有以下训练数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x &= [0, 0, 1, 0, 0] \\\\ \n",
    "y &= [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设计一个只有1个输入层、1个隐藏层、1个输出层的神经网络，其中输出层的神经元个数等于输入层即等于One-hot编码的维度，而隐含层的神经元个数通常远小于输出层，比如One-hot维度如果是10000，隐含层可以只有300个神经元："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过最大化似然函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\prod^{N}_{i=1} \\prod_{-m <= j <= m} \\mathbb{P} \\left( w^{i+j} \\ \\lvert \\  w^i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即对于上下文内所有的词，给定中心词$w^i$，预测滑窗内其他词，越准确越好。对上式取对数并展开："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\prod^{N}_{i=1} \\prod_{-m <= j <= m} \\mathbb{P} \\left( w^{i+j} \\ \\lvert \\  w^i \\right) &= \\sum^{N}_{i=1} \\sum_{-m <= j <= m} \\log \\mathbb{P} \\left( w^{i+j} \\ \\lvert \\  w^i \\right) \\\\\n",
    "&= \\sum^{N}_{i=1} \\sum_{-m <= j <= m} \\log \\left( \\frac{\\exp(\\mathrm{u^T_{i+j} \\cdot v_{i}} )}{ \\mathrm{\\sum^{N}_{k=1} \\exp(\\mathrm{u^T_{k} \\cdot v_{i}})}} \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$\\mathrm{v_i}$即是隐藏层的权重，也是隐藏层的输入$z_i$，也是第i个词的词向量，$\\mathrm{u_{i+j}}$是输出层的权重，也是第i+j个词的词向量的另一个表达。最大化上式的最大似然函数，即最小化下式交叉熵："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "- \\sum^{N}_{i=1} \\mathrm{y_i} \\cdot \\log \\mathrm{p_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\mathrm{y_i}$与$\\mathrm{p_i}$是维度为词表长度的向量，分别代表观测值与计算值，对$\\mathrm{v_i}$求梯度有："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac {\\partial \\log \\mathbb{P} \\left( w^{j} \\ \\lvert \\  w^i \\right)} {\\mathrm{v_i}} &= \\frac {\\partial \\log \\left( \\exp(\\mathrm{u^T_{j} \\cdot v_{i}} ) \\right) - \\log \\left (  \\mathrm{\\sum^{V}_{k=1} \\exp(\\mathrm{u^T_{k} \\cdot v_{i}})} \\right)}{\\partial \\mathrm{v_{i}}} \\\\\n",
    "&= \\mathrm{u_{j}} - \\frac{1}{\\sum^{V}_{w=1} \\exp(\\mathrm{u^T_w v_i}) } \\left[ \\sum^{V}_{k=1} \\exp(\\mathrm{u^T_k v_i) \\cdot \\mathrm{u_k}} \\right] \\\\\n",
    "&= \\mathrm{u_{j}} - \\sum^{V}_{k=1} \\frac{ \\exp(\\mathrm{u^T_k v_i}) }{ \\sum^{V}_{w=1} \\exp(\\mathrm{u^T_w v_i}) } \\cdot \\mathrm{u_k}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用梯度下降更新$\\mathrm{v_i}$，此处的$\\mathrm{v_i}$是向量，在网络中，即是输入层的第i个神经元到隐含层的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即Continuous Bag of Words，连续词袋模型，其核心思想是对于一个上下文，设定一个大小为$m$的滑窗，在滑窗内选择$1$个背景词，$m - 1$个中心词，与Skip-Gram相反，设定滑窗大小为$2$，如果选择中心词$\\ I,\\ eat,\\ every,\\ day$，那么将会有以下训练数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x &= [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1] \\\\ \n",
    "y &= [0, 0, 1, 0, 0] \\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于概率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P} \\left( w^{j} \\ \\lvert \\ w^{i-m}, \\cdots,  w^i, \\cdots, w^{i+m} \\right) =  \\frac{\\exp(\\mathrm{u^T_{j} \\cdot (v_{i} + \\cdots + v_{i+2m}} ) / 2m)}{ \\mathrm{\\sum^{V}_{k=1} \\exp(\\mathrm{u^T_{k} \\cdot (v_{i} + \\cdots + v_{i+2m}} ) / 2m})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与Skip-Gram的不同之处在于将中心词求和后平均，之后的梯度计算与更新和Skip-Gram相同，这里就不展开了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 负采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直观地从上面的梯度更新公式中看到，每一次更新都伴随着巨量的计算开销，这个计算开销主要是因为Softmax函数的分母。可以使用负采样替换Softmax，减少计算开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对于原条件概率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P} \\left( w^{j} \\ \\lvert \\  w^i \\right) =  \\frac{\\exp(\\mathrm{u^T_{j} \\cdot v_{i}} )}{ \\mathrm{\\sum^{V}_{k=1} \\exp(\\mathrm{u^T_{k} \\cdot v_{i}})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将被改写为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P} \\left( w^{j} \\ \\lvert \\  w^i \\right) = \\log \\frac{1}{1 + \\exp(- \\mathrm{u^T_j v_i})} + \\sum^{K}_{k=1} \\log \\left( 1 - \\frac{1}{1 + \\exp(- \\mathrm{u^T_k v_i})} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即筛选出K个不在滑窗内的词向量，直观地理解是希望中心词尽可能地不预测出这些采样出的词，筛选出某个词的概率由这个公式决定："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{P(w_i)} = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum^{V}_{k=1}f(w_k)^{\\frac{3}{4}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$f(w_i)$是这个单词在上下文中出现的频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这种词嵌入模型训练出的词向量能较好的表示两个相近意思的词的近似程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
